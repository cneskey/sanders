#' Web-scraping and Web-crawling Content Parsing, Validation and Sanitization Helpers
#'
#' When researchers crawl/scrape the web for content they are talking to
#' strange computers running software created by a diverse array of humans. Content --
#' even content that is supposed to adhere to internet stadards -- can have very
#' rough edges that need to be smoothed out to be useful and potentially less harmful
#' to the systems running the scraping and analysis code. Methods are provided that
#' sand off the rough edges of many different types of scraped content and metadata.
#'
#' @name sanders
#' @docType package
#' @author Bob Rudis (bob@@rud.is)
#' @import tibble stringi
NULL
